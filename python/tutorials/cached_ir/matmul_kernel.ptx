//
// Generated by LLVM NVPTX Back-End
//

.version 7.8
.target sm_86
.address_size 64

	// .globl	matmul_kernel_0d1d2d3d4d5d6d7c8d9c10d11c
.extern .shared .align 1 .b8 global_smem[];

.visible .entry matmul_kernel_0d1d2d3d4d5d6d7c8d9c10d11c(
	.param .u64 matmul_kernel_0d1d2d3d4d5d6d7c8d9c10d11c_param_0,
	.param .u64 matmul_kernel_0d1d2d3d4d5d6d7c8d9c10d11c_param_1,
	.param .u64 matmul_kernel_0d1d2d3d4d5d6d7c8d9c10d11c_param_2,
	.param .u32 matmul_kernel_0d1d2d3d4d5d6d7c8d9c10d11c_param_3,
	.param .u32 matmul_kernel_0d1d2d3d4d5d6d7c8d9c10d11c_param_4,
	.param .u32 matmul_kernel_0d1d2d3d4d5d6d7c8d9c10d11c_param_5,
	.param .u32 matmul_kernel_0d1d2d3d4d5d6d7c8d9c10d11c_param_6,
	.param .u32 matmul_kernel_0d1d2d3d4d5d6d7c8d9c10d11c_param_7,
	.param .u32 matmul_kernel_0d1d2d3d4d5d6d7c8d9c10d11c_param_8
)
.maxntid 128, 1, 1
{
	.reg .pred 	%p<9>;
	.reg .b16 	%h<9>;
	.reg .b32 	%r<270>;
	.reg .b32 	%hh<41>;
	.reg .f32 	%f<89>;
	.reg .b64 	%rd<33>;

	ld.param.u32 	%r49, [matmul_kernel_0d1d2d3d4d5d6d7c8d9c10d11c_param_8];
	ld.param.u32 	%r48, [matmul_kernel_0d1d2d3d4d5d6d7c8d9c10d11c_param_4];
	ld.param.u32 	%r47, [matmul_kernel_0d1d2d3d4d5d6d7c8d9c10d11c_param_3];
	ld.param.u64 	%rd13, [matmul_kernel_0d1d2d3d4d5d6d7c8d9c10d11c_param_2];
	ld.param.u64 	%rd12, [matmul_kernel_0d1d2d3d4d5d6d7c8d9c10d11c_param_1];
	ld.param.u64 	%rd11, [matmul_kernel_0d1d2d3d4d5d6d7c8d9c10d11c_param_0];
	mov.u32 	%r68, %tid.x;
	bfe.u32 	%r1, %r68, 2, 3;
	shr.u32 	%r69, %r68, 2;
	ld.param.u32 	%r70, [matmul_kernel_0d1d2d3d4d5d6d7c8d9c10d11c_param_5];
	and.b32  	%r71, %r69, 24;
	ld.param.u32 	%r72, [matmul_kernel_0d1d2d3d4d5d6d7c8d9c10d11c_param_6];
	or.b32  	%r2, %r71, %r1;
	ld.param.u32 	%r73, [matmul_kernel_0d1d2d3d4d5d6d7c8d9c10d11c_param_7];
	and.b32  	%r3, %r68, 3;
	shl.b32 	%r4, %r3, 3;
	mov.u32 	%r74, %ctaid.x;
	add.s32 	%r75, %r47, 31;
	shr.s32 	%r76, %r75, 31;
	shr.u32 	%r77, %r76, 27;
	add.s32 	%r78, %r75, %r77;
	shr.s32 	%r79, %r78, 5;
	add.s32 	%r80, %r48, 31;
	shr.s32 	%r81, %r80, 31;
	shr.u32 	%r82, %r81, 27;
	add.s32 	%r83, %r80, %r82;
	shr.s32 	%r84, %r83, 5;
	add.s32 	%r85, %r70, 31;
	shl.b32 	%r89, %r84, 2;
	div.s32 	%r92, %r74, %r89;
	shl.b32 	%r93, %r92, 2;
	sub.s32 	%r94, %r79, %r93;
	min.s32 	%r95, %r94, 4;
	rem.s32 	%r96, %r74, %r95;
	add.s32 	%r97, %r93, %r96;
	mul.lo.s32 	%r98, %r92, %r89;
	sub.s32 	%r99, %r74, %r98;
	div.s32 	%r100, %r99, %r95;
	shl.b32 	%r101, %r97, 5;
	or.b32  	%r6, %r101, %r2;
	shl.b32 	%r102, %r100, 5;
	or.b32  	%r7, %r102, %r4;
	mad.lo.s32 	%r103, %r6, %r72, %r4;
	mul.wide.s32 	%rd18, %r103, 2;
	add.s64 	%rd14, %rd11, %rd18;
	mad.lo.s32 	%r104, %r2, %r73, %r7;
	mul.wide.s32 	%rd19, %r104, 2;
	add.s64 	%rd15, %rd12, %rd19;
	shl.b32 	%r105, %r73, 5;
	setp.lt.s32 	%p1, %r85, 32;
	setp.gt.s32 	%p2, %r85, 31;
	shl.b32 	%r106, %r2, 5;
	and.b32  	%r107, %r68, 24;
	xor.b32  	%r108, %r4, %r107;
	or.b32  	%r8, %r106, %r108;
	shl.b32 	%r109, %r8, 1;
	mov.u32 	%r131, global_smem;
	add.s32 	%r50, %r131, %r109;
	selp.b32 	%r51, 16, 0, %p2;
	cp.async.cg.shared.global [ %r50 + 0 ], [ %rd14 + 0 ], 0x10, %r51;
	cp.async.commit_group ;
	add.s32 	%r264, %r131, 6144;
	add.s32 	%r52, %r264, %r109;
	cp.async.cg.shared.global [ %r52 + 0 ], [ %rd15 + 0 ], 0x10, %r51;
	cp.async.commit_group ;
	add.s64 	%rd16, %rd14, 64;
	mul.wide.s32 	%rd20, %r105, 2;
	add.s64 	%rd17, %rd15, %rd20;
	setp.gt.s32 	%p3, %r85, 63;
	bar.sync 	0;
	add.s32 	%r54, %r50, 2048;
	selp.b32 	%r55, 16, 0, %p3;
	cp.async.cg.shared.global [ %r54 + 0 ], [ %rd16 + 0 ], 0x10, %r55;
	cp.async.commit_group ;
	add.s32 	%r56, %r50, 8192;
	cp.async.cg.shared.global [ %r56 + 0 ], [ %rd17 + 0 ], 0x10, %r55;
	cp.async.commit_group ;
	cp.async.wait_group 0x2;
	bar.sync 	0;
	bfe.u32 	%r9, %r68, 4, 1;
	shr.u32 	%r112, %r68, 1;
	bfe.u32 	%r10, %r68, 1, 2;
	and.b32  	%r11, %r112, 16;
	and.b32  	%r12, %r68, 15;
	or.b32  	%r13, %r12, %r11;
	xor.b32  	%r113, %r9, %r10;
	shl.b32 	%r114, %r13, 5;
	shl.b32 	%r115, %r113, 3;
	or.b32  	%r14, %r114, %r115;
	shl.b32 	%r116, %r14, 1;
	add.s32 	%r62, %r131, %r116;
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r258, %r259, %r260, %r261 }, [ %r62 + 0 ];
	shr.u32 	%r15, %r68, 6;
	bfe.u32 	%r117, %r68, 6, 1;
	shl.b32 	%r118, %r9, 1;
	or.b32  	%r16, %r118, %r117;
	xor.b32  	%r119, %r16, %r10;
	shl.b32 	%r120, %r12, 5;
	shl.b32 	%r121, %r119, 3;
	or.b32  	%r17, %r121, %r120;
	shl.b32 	%r122, %r17, 1;
	add.s32 	%r67, %r264, %r122;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r254, %r255, %r256, %r257 }, [ %r67 + 0 ];
	mov.f32 	%f73, 0f00000000;
	mov.f32 	%f74, %f73;
	mov.f32 	%f75, %f73;
	mov.f32 	%f76, %f73;
	mov.f32 	%f77, %f73;
	mov.f32 	%f78, %f73;
	mov.f32 	%f79, %f73;
	mov.f32 	%f80, %f73;
	@%p1 bra 	LBB0_3;
	shr.s32 	%r86, %r85, 31;
	shr.u32 	%r87, %r86, 27;
	add.s32 	%r88, %r85, %r87;
	shr.s32 	%r5, %r88, 5;
	cvt.s64.s32 	%rd1, %r103;
	cvt.s64.s32 	%rd2, %r104;
	cvt.s64.s32 	%rd3, %r105;
	shl.b64 	%rd4, %rd3, 1;
	add.s64 	%rd21, %rd4, %rd2;
	shl.b64 	%rd22, %rd21, 1;
	add.s64 	%rd32, %rd12, %rd22;
	shl.b64 	%rd23, %rd1, 1;
	add.s64 	%rd24, %rd23, %rd11;
	add.s64 	%rd31, %rd24, 128;
	mov.f32 	%f73, 0f00000000;
	mov.u32 	%r266, 0;
	mov.u32 	%r269, 1;
	mov.u32 	%r268, 32;
	mov.u32 	%r263, 2;
	mov.u32 	%r253, %r131;
	mov.u32 	%r262, %r269;
	mov.u32 	%r267, %r131;
	mov.f32 	%f74, %f73;
	mov.f32 	%f75, %f73;
	mov.f32 	%f76, %f73;
	mov.f32 	%f77, %f73;
	mov.f32 	%f78, %f73;
	mov.f32 	%f79, %f73;
	mov.f32 	%f80, %f73;
LBB0_2:
	mul.hi.u32 	%r184, %r262, -1431655765;
	shr.u32 	%r185, %r184, 1;
	mul.lo.s32 	%r186, %r185, 6144;
	mov.u32 	%r187, 8192;
	sub.s32 	%r188, %r187, %r186;
	mul.hi.u32 	%r189, %r263, -1431655765;
	shr.u32 	%r190, %r189, 1;
	add.s32 	%r191, %r266, 16;
	shl.b32 	%r192, %r269, 5;
	add.s32 	%r193, %r267, %r192;
	shl.b32 	%r194, %r268, 5;
	add.s32 	%r195, %r264, %r194;
	shr.u32 	%r196, %r191, 3;
	add.s32 	%r197, %r196, %r9;
	xor.b32  	%r198, %r197, %r10;
	shl.b32 	%r199, %r198, 3;
	mad.lo.s32 	%r200, %r268, %r13, %r199;
	mov.u32 	%r201, -16;
	sub.s32 	%r202, %r201, %r266;
	shl.b32 	%r203, %r202, 1;
	add.s32 	%r204, %r193, %r203;
	shl.b32 	%r205, %r200, 1;
	add.s32 	%r140, %r204, %r205;
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r158, %r159, %r160, %r161 }, [ %r140 + 0 ];
	shr.u32 	%r206, %r266, 3;
	add.s32 	%r207, %r206, %r16;
	xor.b32  	%r208, %r207, %r10;
	shl.b32 	%r209, %r208, 3;
	mad.lo.s32 	%r210, %r268, %r12, %r209;
	shl.b32 	%r211, %r266, 1;
	sub.s32 	%r212, %r195, %r211;
	shl.b32 	%r213, %r210, 1;
	add.s32 	%r145, %r212, %r213;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r141, %r142, %r143, %r144 }, [ %r145 + 0 ];
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f73, %f74, %f75, %f76 }, { %r258, %r259, %r260, %r261 }, { %r254, %r255 }, { %f73, %f74, %f75, %f76 };
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f77, %f78, %f79, %f80 }, { %r258, %r259, %r260, %r261 }, { %r256, %r257 }, { %f77, %f78, %f79, %f80 };
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f73, %f74, %f75, %f76 }, { %r158, %r159, %r160, %r161 }, { %r141, %r142 }, { %f73, %f74, %f75, %f76 };
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 { %f77, %f78, %f79, %f80 }, { %r158, %r159, %r160, %r161 }, { %r143, %r144 }, { %f77, %f78, %f79, %f80 };
	add.s32 	%r262, %r262, 1;
	setp.lt.s32 	%p4, %r263, %r5;
	bar.sync 	0;
	add.s32 	%r214, %r253, %r109;
	mad.lo.s32 	%r215, %r190, -6144, %r214;
	add.s32 	%r170, %r215, 4096;
	selp.b32 	%r171, 16, 0, %p4;
	cp.async.cg.shared.global [ %r170 + 0 ], [ %rd31 + 0 ], 0x10, %r171;
	cp.async.commit_group ;
	add.s32 	%r172, %r215, 10240;
	cp.async.cg.shared.global [ %r172 + 0 ], [ %rd32 + 0 ], 0x10, %r171;
	cp.async.commit_group ;
	cp.async.wait_group 0x2;
	bar.sync 	0;
	add.s32 	%r264, %r253, %r188;
	add.s32 	%r267, %r264, -6144;
	mov.u32 	%r266, 0;
	add.s32 	%r45, %r263, 1;
	add.s32 	%r216, %r253, %r116;
	add.s32 	%r217, %r216, %r188;
	add.s32 	%r178, %r217, -6144;
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r258, %r259, %r260, %r261 }, [ %r178 + 0 ];
	add.s32 	%r218, %r253, %r122;
	add.s32 	%r183, %r218, %r188;
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r254, %r255, %r256, %r257 }, [ %r183 + 0 ];
	add.s32 	%r253, %r253, 2048;
	add.s64 	%rd32, %rd32, %rd4;
	add.s64 	%rd31, %rd31, 64;
	add.s32 	%r219, %r263, -1;
	setp.lt.s32 	%p5, %r219, %r5;
	mov.u32 	%r263, %r45;
	@%p5 bra 	LBB0_2;
LBB0_3:
	cp.async.wait_group 0x0;
	bar.sync 	0;
	cvt.rn.f16.f32 	%h1, %f73;
	cvt.rn.f16.f32 	%h2, %f74;
	cvt.rn.f16.f32 	%h3, %f75;
	cvt.rn.f16.f32 	%h4, %f76;
	cvt.rn.f16.f32 	%h5, %f77;
	cvt.rn.f16.f32 	%h6, %f78;
	cvt.rn.f16.f32 	%h7, %f79;
	cvt.rn.f16.f32 	%h8, %f80;
	mul.lo.s32 	%r224, %r6, %r49;
	mul.wide.s32 	%rd28, %r224, 2;
	add.s64 	%rd29, %rd13, %rd28;
	mul.wide.s32 	%rd30, %r7, 2;
	add.s64 	%rd27, %rd29, %rd30;
	setp.lt.s32 	%p7, %r6, %r47;
	setp.lt.s32 	%p8, %r7, %r48;
	and.pred  	%p6, %p7, %p8;
	shl.b32 	%r225, %r3, 1;
	or.b32  	%r226, %r11, %r1;
	shl.b32 	%r227, %r15, 3;
	and.b32  	%r228, %r227, 24;
	or.b32  	%r229, %r228, %r225;
	mad.lo.s32 	%r230, %r226, 40, %r229;
	shl.b32 	%r231, %r230, 1;
	add.s32 	%r233, %r131, %r231;
	st.shared.v2.b16 	[%r233], {%h1, %h2};
	st.shared.v2.b16 	[%r233+640], {%h3, %h4};
	st.shared.v2.b16 	[%r233+32], {%h5, %h6};
	st.shared.v2.b16 	[%r233+672], {%h7, %h8};
	bar.sync 	0;
	mad.lo.s32 	%r234, %r2, 40, %r4;
	shl.b32 	%r235, %r234, 1;
	add.s32 	%r236, %r131, %r235;
	ld.shared.v4.u32 	{%r220, %r221, %r222, %r223}, [%r236];
	@%p6 st.global.v4.b32 [ %rd27 + 0 ], { %r220, %r221, %r222, %r223 };
	ret;

}
